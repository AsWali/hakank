/*
  Linear regression.

  Model inspired by https://www.microsoft.com/en-us/research/wp-content/uploads/2014/02/john-winn.pdf
  page 9
  
*/

// This is much faster than linear_regression.psi, and don't crash with --expectation!



// First, we don't try to calculate the error term (sigma).

// for n = 10:
// With a = b = gauss(2,2):
// E[a_,b_] = (801062267/404750000,1112958639/404750000)
// (~1.91835423319481789294, ~3.17532049278904913224) 
//
// With a = b = gauss(4,4)
// E[a_,b_] = (866727373/461906250,3212846447/923812500)
// (~1.87641404072796157229, ~3.47781226926459644138)
// Well, not too bad!

// for n = 20:
// E[a_,b_] = (2658932720003/1330578000400,27704108381869/6652890002000)
// (~1.99832908645992070019, ~4.16422162000883176484)
// It took 2.992s
// Quite good!

// for n=100
// With --mathematica --nocheck (but no --expectation) it took 1min19s
// p[a_,b_] := 1/2000*1/Pi*2083294225001^(1/2)*E^(-169175001/2000*a^2+-2525*a*b+-50001/2000*b^2+-5978345925031881685960833649/16666353800008000000000+139332800309/400000*a+2624181983/500000*b)

// With --mathematica --nocheck --expectation it took 1min 17s
// E[a_,b_] = (1665931742590309/833317690000400,4176838622925733/1041647112500500)
// (~1.99915561925669589200,~4.00984035072984285724)


// Now, we also add the error term sigma = gauss(0,1)
// When using beta(2,2) or uniform(0,1) as error term --expectation don't return proper numbers

// n = 10
// 
// E[a,b,sigma] = (388093945221/206555002750,712067927587/206555002750,712067927587/206555002750000)
// (~1.87888910969986177205,~3.44735260878110104259,~0.00344735260878110104)
// 0.615s

// n=20
// E[a,b,sigma] = (1064371744513/532647600840,18469237940623/4438730007000,18469237940623/4438730007000000)
// (~1.99826628869529557159,~4.16092844383337145831,~0.00416092844383337146)
// It took 2.074s

// n=100 (--nocheck --expectation)
// E[a_,b_,sigma_] = (3335055737199889/1668234190020200,8353674621669483/2085292737525250,8353674621669483/2085292737525250000)
// (~1.99915321071288324641,~4.00599612291525165200,~0.00400599612291525165)
// It took 52.233s
// With new-types: 8min38s!!!
//
def main() {
   /* generated by R:
      n = 10
      x <- 1:n
      y <- 4+2*x+rnorm(n,0,0.6)

      ie. y = b + a*x + random_noise
   */

   // For n = 10
   x := [1,2,3,4,5,6,7,8,9,10];
   y := [5.617247,7.567268,9.133846,11.969845,13.422352,16.637612, 7.386774,20.331703,22.000001,23.783700];

   // For n = 20
   // x := [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20];
   // y := [6.379492,8.321083,9.969468,12.495791,14.241795,15.859028,17.597066,
   //       20.146905,22.197051,24.015312,26.082544,28.689255,30.665810,31.832002,
   //       33.977692,35.574370,38.189304,39.845913,41.884899,44.977089];

   // For n = 100;
   // x := [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,
   //       19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,
   //       37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,
   //       55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,
   //       73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,
   //       91,92,93,94,95,96,97,98,99,100];
   // y := [4.996549, 7.580813, 9.648042,11.641315,13.947475,17.210101,
   //       17.168745,19.658700,22.001391,22.918219,26.354980,27.020636,
   //       30.618755,31.605082,33.581192,36.093840,37.562423,40.013625,
   //       42.804447,43.578837,45.720999,48.360063,51.114672,52.897811,
   //       54.157389,55.950914,57.939275,60.932903,62.154241,64.440353,
   //       66.101312,67.207347,70.337320,72.334169,74.433569,75.396752,
   //       77.755557,79.090233,81.594088,85.166582,85.540334,87.252934,
   //       90.054751,93.158092,93.733585,95.902528,97.551510,99.520743,
   //       103.127475,104.044051,105.600355,108.856979,110.795045,112.407349,
   //       113.920185,116.756648,118.323365,119.739957,122.877446,123.821252,
   //       126.733599,128.074933,129.387535,131.510970,133.656934,135.967224,
   //       137.801790,139.816715,140.726086,143.068037,146.686390,148.269513,
   //       149.594165,152.307272,154.103876,156.558550,157.774781,160.942264,
   //       161.524941,163.547719,165.733934,167.766419,170.845981,171.509630,
   //       174.795970,175.776202,177.746042,178.981861,181.814310,184.839890,
   //       185.875081,187.383972,190.050468,191.080974,194.095335,196.536313,
   //       197.305674,199.578121,201.185981,203.697180];


   len := y.length;

   // a := gauss(2,2);
   // b := gauss(2,2);

   // a := gauss(4,2);
   // b := gauss(4,2);

   // a := gauss(4,4);
   // b := gauss(4,4);

   a := gauss(0,1000);
   b := gauss(0,1000);

   // sigma := beta(2,2);
   // sigma := uniform(0,1);
   sigma := gauss(0,1);

   for i in [0..len-1] {
     // clean_y := a * x[i] + b; 
     // cobserve(y[i], gauss(clean_y,1));
     // cobserve(y[i], gauss(clean_y,4));
     
     // This works as well:
     // cobserve(y[i],gauss(a* x[i] + b,1) + gauss(0,1));

     // Without noise term: almost the same result
     // cobserve(y[i],gauss(a* x[i] + b,1));
     

     // Testing with sigma:
     cobserve(y[i],gauss(a* x[i] + b,1) + sigma);


     // This version (i.e. sigma in gaus) is much slower
     // and seg faults (as in linear_regression.psi)
     // cobserve(y[i],gauss(a* x[i] + b,sigma));     
     


   }

   return(a,b,sigma);
   
}